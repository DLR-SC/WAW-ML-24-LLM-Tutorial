{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import requests\n",
    "import json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the book \"20,000 Leagues Under the Sea\" by Jules Verne from the Gutenberg project and save it localy.\n",
    "Download link: \"https://www.gutenberg.org/cache/epub/164/pg164.txt\"\n",
    "<!--  -->\n",
    "Load the text file and remove the introduction as well as the table of contents."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# The URL of the text\n",
    "url = \"https://www.gutenberg.org/cache/epub/164/pg164.txt\"\n",
    "\n",
    "# Send a GET request to the URL\n",
    "response = requests.get(url)\n",
    "\n",
    "# Check if the request was successful (status code 200)\n",
    "if response.status_code == 200:\n",
    "    # Save the content to a text file\n",
    "    with open(\"TwentyThousandLeaguesUnderTheSea-JulesVerne.txt\", \"w\", encoding=\"utf-8\") as file:\n",
    "        file.write(response.text)\n",
    "    print(\"Download successful, content saved to 'TwentyThousandLeaguesUnderTheSea-JulesVerne.txt'\")\n",
    "else:\n",
    "    print(f\"Failed to download the file. Status code: {response.status_code}\")\n",
    "\n",
    "# Open the file and load its contents into a string called docs\n",
    "with open(\"TwentyThousandLeaguesUnderTheSea-JulesVerne.txt\", \"r\", encoding=\"utf-8\") as file:\n",
    "    docs = file.read()\n",
    "    # Remove empty lines from the string\n",
    "    docs = \"\\n\".join([line for line in docs.splitlines() if line.strip()])\n",
    "    # Remove introduction and ToC \n",
    "    docs = docs[2700:]\n",
    "\n",
    "# Check the first few characters to confirm it's loaded\n",
    "print(docs[:500]) \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunk documents or textfile(s) into chunks! Experiment with chunk size as well as chunk overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the text splitter to divide text into chunks\n",
    "# - `chunk_size` is the maximum size of each chunk (1000 characters in this case)\n",
    "# - `chunk_overlap` is the number of characters that will overlap between chunks (100 characters)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "\n",
    "# Code for processing text files\n",
    "# Split the provided text documents into chunks using the defined text splitter\n",
    "docs = text_splitter.split_text(docs)\n",
    "\n",
    "# Print the number of chunks created from the documents\n",
    "print(f\"Current number of chunks {len(docs)}.\")\n",
    "print(docs[0])  # Uncomment to print the first chunk for verification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Initialize model for converting text into numerical representation (embeddings). We are using all-MiniLM-L6-v2 from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the embedding model to convert text into embeddings\n",
    "# Using the \"all-MiniLM-L6-v2\" model from the Sentence Transformers library\n",
    "embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embedding_function = HuggingFaceEmbeddings(model_name=embedding_model, model_kwargs={'device': 'cpu'})\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " ###  Convert & store PDF chunks in our vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vector store for storing text embeddings (for text files)\n",
    "# Use the embedding function defined earlier\n",
    "vectorstore = Chroma.from_texts(docs, embedding_function, persist_directory=\"./chroma_db_text\")\n",
    "\n",
    "# Print the number of entries in the vector store\n",
    "print(vectorstore._collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formulate a question and retrieve top k similar documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example questions for retrieval (uncomment one at a time to use)\n",
    "\n",
    "# question = \"Who is Ned Land, and what is his expertise?\"\n",
    "# question = \"What tactics does the ship Abraham Lincoln use to track down the sea creature?\"\n",
    "question = \"What dangers do Professor Aronnax and the others face in the forest?\"\n",
    "\n",
    "# Convert the vector store into a retriever object to search for similar documents\n",
    "# Using a similarity-based search with the top 3 similar results\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "retrieved_docs = retriever.invoke(question)\n",
    "\n",
    "# Print the retrieved documents and the number of similar documents found\n",
    "print(retrieved_docs)\n",
    "print(f\"Collected most {len(retrieved_docs)} similar documents.\")\n",
    "\n",
    "# Function to format the retrieved documents into a readable format\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Create a context from the formatted documents\n",
    "context = format_docs(retrieved_docs)\n",
    "print(context)  # Uncomment to print the formatted context for verification\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide question and context to the LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define headers for the HTTP request to indicate we are sending JSON data\n",
    "headers = { \"Content-Type\": \"application/json\" }\n",
    "\n",
    "# Combine the user query and context into a single prompt\n",
    "user_query = f\"\\nQuestion: {question}\\nContext: {context}\"\n",
    "\n",
    "# Define the JSON payload for the POST request\n",
    "data = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"assistant\", \"content\": \"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\"},\n",
    "        {\"role\": \"user\", \"content\": user_query}\n",
    "    ],\n",
    "    \"temperature\": 0.7,  # Controls the randomness of the output\n",
    "    \"max_tokens\": -1,     # Maximum number of tokens in the output (unlimited in this case)\n",
    "    \"stream\": False       # Whether to stream the output or not\n",
    "}\n",
    "\n",
    "# Make a POST request to the local server running at localhost:1234\n",
    "response = requests.post(\"http://localhost:1234/v1/chat/completions\", headers=headers, data=json.dumps(data))\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Extract the assistant's response from the JSON response\n",
    "    # bot_response = response.json()  # Uncomment to see the full response\n",
    "    bot_response = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    print(\"Answer:\", bot_response)\n",
    "else:\n",
    "    # Print an error message if the request failed\n",
    "    print(\"Failed to get response:\", response.status_code, response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
