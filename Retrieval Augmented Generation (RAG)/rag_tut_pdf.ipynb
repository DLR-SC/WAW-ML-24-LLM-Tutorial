{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "from langchain_community.embeddings import HuggingFaceEmbeddings\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "from langchain_community.document_loaders import PyPDFLoader\n",
    "import requests\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download the following PDF documents (or use you own):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define the URLs and filenames\n",
    "pdf_urls = [\n",
    "    \"https://link.springer.com/content/pdf/10.1186/s13000-024-01464-7.pdf\",\n",
    "    \"https://proceedings.neurips.cc/paper_files/paper/2023/file/91f18a1287b398d378ef22505bf41832-Paper-Datasets_and_Benchmarks.pdf\"\n",
    "]\n",
    "\n",
    "pdf_filenames = [\n",
    "    \"Papers/Paper1.pdf\",\n",
    "    \"Papers/Paper2.pdf\"\n",
    "]\n",
    "\n",
    "# Make sure the \"Papers\" directory exists\n",
    "directory = \"Papers\"\n",
    "if not os.path.exists(directory):\n",
    "    os.makedirs(directory)\n",
    "\n",
    "# Download and save the PDF files locally\n",
    "for pdf_url, pdf_filename in zip(pdf_urls, pdf_filenames):\n",
    "    response = requests.get(pdf_url)\n",
    "    with open(pdf_filename, \"wb\") as file:\n",
    "        file.write(response.content)\n",
    "    print(f\"Downloaded and saved {pdf_filename}\")\n",
    "\n",
    "# Initialize an empty list to store all documents\n",
    "all_docs = []\n",
    "\n",
    "# Load the downloaded PDF files using PyPDFLoader\n",
    "for pdf_filename in pdf_filenames:\n",
    "    loader = PyPDFLoader(pdf_filename)\n",
    "    docs = loader.load()\n",
    "    all_docs.extend(docs)\n",
    "    print(f\"Pages loaded from {pdf_filename}: {len(docs)}.\")\n",
    "\n",
    "# Output the total number of pages loaded from both documents\n",
    "print(f\"Total pages loaded from both documents: {len(all_docs)}.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chunk document(s) into chunks! Experiment with chunk size as well as chunk overlap"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize the text splitter to divide text into chunks\n",
    "# - `chunk_size` is the maximum size of each chunk (1000 characters in this case)\n",
    "# - `chunk_overlap` is the number of characters that will overlap between chunks (100 characters)\n",
    "text_splitter = RecursiveCharacterTextSplitter(chunk_size=1000, chunk_overlap=100)\n",
    "\n",
    "\n",
    "# Code for processing PDF documents\n",
    "docs = text_splitter.split_documents(all_docs)\n",
    "\n",
    "# Print the number of chunks created from the documents\n",
    "print(f\"Current number of chunks {len(docs)}.\")\n",
    "print(docs[0])  # Uncomment to print the first chunk for verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Initialize model for converting text into numerical representation (embeddings). We are using all-MiniLM-L6-v2 from HuggingFace"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the embedding model to convert text into embeddings\n",
    "# Using the \"all-MiniLM-L6-v2\" model from the Sentence Transformers library\n",
    "embedding_model = \"sentence-transformers/all-MiniLM-L6-v2\"\n",
    "embedding_function = HuggingFaceEmbeddings(model_name=embedding_model, model_kwargs={'device': 'cpu'})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert & store PDF chunks in our vectorstore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a vector store for storing document embeddings (for PDF documents)\n",
    "vectorstore = Chroma.from_documents(docs, embedding_function, persist_directory=\"./chroma_db_docs\")\n",
    "\n",
    "# Print the number of entries in the vector store\n",
    "print(vectorstore._collection.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Formulate a question and retrieve top k similar documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example questions for retrieval (uncomment one at a time to use)\n",
    "\n",
    "# question = \"What are key differences between Image-based and Laboratory-based Diagnostics with a focus on LLM applications?\" # Question for paper 1\n",
    "question = \"What kinds of biases are associated with LLM-as-a-Judge models?\" # Question for paper 2\n",
    "\n",
    "# Convert the vector store into a retriever object to search for similar documents\n",
    "# Using a similarity-based search with the top 3 similar results\n",
    "retriever = vectorstore.as_retriever(search_type=\"similarity\", search_kwargs={\"k\": 3})\n",
    "retrieved_docs = retriever.invoke(question)\n",
    "\n",
    "# Print the retrieved documents and the number of similar documents found\n",
    "print(retrieved_docs)\n",
    "print(f\"Collected most {len(retrieved_docs)} similar documents.\")\n",
    "\n",
    "# Function to format the retrieved documents into a readable format\n",
    "def format_docs(docs):\n",
    "    return \"\\n\\n\".join(doc.page_content for doc in docs)\n",
    "\n",
    "# Create a context from the formatted documents\n",
    "context = format_docs(retrieved_docs)\n",
    "print(context)  # Uncomment to print the formatted context for verification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Provide question and context to the LLM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Define headers for the HTTP request to indicate we are sending JSON data\n",
    "headers = { \"Content-Type\": \"application/json\" }\n",
    "\n",
    "# Combine the user query and context into a single prompt\n",
    "user_query = f\"\\nQuestion: {question}\\nContext: {context}\"\n",
    "\n",
    "# Define the JSON payload for the POST request\n",
    "data = {\n",
    "    \"messages\": [\n",
    "        {\"role\": \"assistant\", \"content\": \"You are an assistant for question-answering tasks. Use the following pieces of retrieved context to answer the question. If you don't know the answer, just say that you don't know. Use three sentences maximum and keep the answer concise.\"},\n",
    "        {\"role\": \"user\", \"content\": user_query}\n",
    "    ],\n",
    "    \"temperature\": 0.7,  # Controls the randomness of the output\n",
    "    \"max_tokens\": -1,     # Maximum number of tokens in the output (unlimited in this case)\n",
    "    \"stream\": False       # Whether to stream the output or not\n",
    "}\n",
    "\n",
    "# Make a POST request to the local server running at localhost:1234\n",
    "response = requests.post(\"http://localhost:1234/v1/chat/completions\", headers=headers, data=json.dumps(data))\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    # Extract the assistant's response from the JSON response\n",
    "    # bot_response = response.json()  # Uncomment to see the full response\n",
    "    bot_response = response.json()[\"choices\"][0][\"message\"][\"content\"]\n",
    "    print(\"Answer:\", bot_response)\n",
    "else:\n",
    "    # Print an error message if the request failed\n",
    "    print(\"Failed to get response:\", response.status_code, response.text)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rag",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
