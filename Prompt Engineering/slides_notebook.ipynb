{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# BEFORE WE START \n",
    "*(Based on the Tutorial's README.MD)*\n",
    "\n",
    "- [x] Miniforge should be installed\n",
    "        \n",
    "        $ conda init powershell  # only for Windows users - requires terminal restart\n",
    "        \n",
    "        $ conda activate \n",
    "        \n",
    "        $ mamba create -n waw_ml python=3.10  # mamba/conda depending on what you use\n",
    "        \n",
    "        $ conda activate waw_ml\n",
    "        \n",
    "        $ pip install -r requirements.txt`\n",
    "\n",
    "NOTE: you might need to restart you VS code \n",
    "- [x] Choose the kernel `waw_ml`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "slideshow": {
     "slide_type": "skip"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "<style>\n",
       "    div.output_area pre {\n",
       "        white-space: pre-wrap;\n",
       "        word-wrap: break-word;\n",
       "    }\n",
       "</style>\n"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Run this piece of code to see the output text wrapped\n",
    " \n",
    "from IPython.display import display, HTML, Markdown\n",
    "# Set CSS for text wrapping in Jupyter notebook\n",
    "display(HTML('''\n",
    "<style>\n",
    "    div.output_area pre {\n",
    "        white-space: pre-wrap;\n",
    "        word-wrap: break-word;\n",
    "    }\n",
    "</style>\n",
    "'''))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "# Please  run this cell ONCE and restart your Kernel.\n",
    "!pip install -qU langchain_mistralai ipywidgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'A9xKQZpgbYi3gMR2uVXUUKu5PBYH5pAK'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from helper.custom_lllm import CustomLLM\n",
    "\n",
    "from langchain_mistralai import ChatMistralAI\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "# Load environment variables from the .env file\n",
    "load_dotenv()\n",
    "\n",
    "# Access the API key using os.getenv\n",
    "api_key = os.getenv(\"MISTRAL_API_KEY\")\n",
    "api_key # You can have your own key on Mistral: https://console.mistral.ai/api-keys/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Loading the Model \n",
    "## (Locally or via API)\n",
    "\n",
    "Make sure to run Mistral7B locally with LMSTUDIO\n",
    "\n",
    "- On the left column menu, select `Developer` (The green icon)\n",
    "- Select the model under **loaded models**: `llm mistral-7b-instruct-v0.2.Q5_K_M.gguf`\n",
    "- On the left side, click the button `Start Server` (Do not change anything in the settings below it)\n",
    "\n",
    "You can see in the Server Logs that the *model* is accessible via  http://localhost:1234/v1/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Loading models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "model_local = CustomLLM() # Load Mistral7b from LM Studio local server \n",
    "\n",
    "model_api = ChatMistralAI(model=\"open-mistral-7b\") # Load Mistral7b or Mixtral8x7B throught the API\n",
    "# Note - The latest `open-mistral-nemo`: https://mistral.ai/news/mistral-nemo/\n",
    "# All models: https://docs.mistral.ai/getting-started/models/\n",
    "\n",
    "# CHOOSE THE MODEL YOU WANT TO USE"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Test Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESPONSE from Local Mistral (With Quantization)\n",
      "-----------------------------------------------\n",
      " DLR stands for Deutsches Zentrum für Luft- und Raumfahrt, which translates to the German Aerospace Center. It is a research center for aeronautics and spaceflight based in Germany. The organization's mission includes research and development activities in various areas of aerospace technology, including aircraft design, engine development, space exploration, and Earth observation. DLR operates its own research facilities, conducts research in collaboration with universities and industry partners, and also participates in international space programs such as those managed by NASA and the European Space Agency (ESA).\n"
     ]
    }
   ],
   "source": [
    "# Create a prompt\n",
    "prompt = '''\n",
    "    What is DLR?\n",
    "    '''\n",
    "\n",
    "# Chain: contains the mode and the output\n",
    "chain =   model_local | StrOutputParser()\n",
    "\n",
    "response = chain.invoke(prompt)\n",
    "print(\"RESPONSE from Local Mistral (With Quantization)\")\n",
    "print(\"-\"*len(\"RESPONSE from Local Mistral (With Quantization)\"))\n",
    "print(response)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"images/doit.png\" alt=\"drawing\" width=\"24px\"/>  `TRYITYourself_1`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RESPONSE using Mistral through API\n",
      "----------------------------------\n",
      "DLR stands for Deutsches Zentrum für Luft- und Raumfahrt e.V., which translates to German Aerospace Center in English. It is a research center for aerospace, energy, transportation, and digitalization. DLR is a federal research center for aerospace, energy, transportation, and digitalization. Its mission is to conduct research and development work in aeronautics, space, energy, transport, digitalization, and security, and to promote the application of its results for the benefit of society. It is headquartered in Berlin, Germany.\n"
     ]
    }
   ],
   "source": [
    "#  <TRYITYourself_1>: RERUN with Mistral API\n",
    "chain_api =   model_api | StrOutputParser()  # REDEFINE CHAIN  # SOL  model_api | StrOutputParser()\n",
    "response = chain_api.invoke(prompt) # Invoke\n",
    "print(\"RESPONSE using Mistral through API\")\n",
    "print(\"-\"*len(\"RESPONSE using Mistral through API\"))\n",
    "\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Section I. Introduction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Generative AI\n",
    "   A machine that is capable of creating content that mimics or approximates <b>human ability</b>. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "   The machine learning models that underpin generative AI have learned these abilities by finding **statistical patterns** in massive datasets of content that was originally generated by humans. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "   Large language models have been trained on trillions of words over many weeks and months, and with large amounts of compute power.\n",
    "\n",
    "[Generative AI from deeplearning.ai](https://www.coursera.org/learn/generative-ai-with-llms/lecture/IrsEw/generative-ai-llms)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Showcasing the capabilities of Generative AI\n",
    "### Examples - Explanation\n",
    "\n",
    "<img src=\"images/example1.png\" alt=\"drawing\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "### Examples - Creative Writing\n",
    "\n",
    "<img src=\"images/example2.png\" alt=\"drawing\"/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Examples - Idea Generation\n",
    "\n",
    "<img src=\"images/example3.png\" alt=\"drawing\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## More Examples\n",
    "\n",
    "> **Summarize** the following text with the focus on `{aspect}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> **Translate** the following text to informal German: `{text}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "> **Create a python function** to `{function requirements}`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"images/doit.png\" alt=\"drawing\" width=\"24px\"/>  `TRYITYourself_2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To create a Python function `process_data` that fills missing values, normalizes numerical columns, and returns the processed DataFrame along with basic statistics, you can make use of the NumPy and pandas libraries. Here's an example implementation:\n",
      "\n",
      "```python\n",
      "import numpy as np\n",
      "import pandas as pd\n",
      "\n",
      "def process_data(dataframe):\n",
      "    # Fill missing values using median for numerical columns and mode for categorical ones\n",
      "    dataframe = dataframe.fillna({np.numerictype(col): dataframe[col].median()\n",
      "                                for col in dataframe.select_dtypes(include='float64, int64').columns\n",
      "                                if not dataframe[col].isnull().sum().sum() == 0} |\n",
      "                               {np.object_dtype: dataframe[dataframe.apply(lambda x: x.isna().all())].mode().iloc[0]})\n",
      "\n",
      "    # Normalize numerical columns using z-score normalization\n",
      "    dataframe_num = dataframe.select_dtypes(include='float64, int64')\n",
      "    dataframe_num_norm = (dataframe_num - dataframe_num.mean()) / dataframe_num.std()\n",
      "    dataframe = pd.concat([dataframe.drop(columns=dataframe_num.columns), dataframe_num_norm], axis=1)\n",
      "    dataframe.columns = dataframe.columns.astype(str).str.replace('^', '_')  # Rename columns with leading numbers to have an underscore prefix\n",
      "\n",
      "    # Basic statistics computation\n",
      "    mean = dataframe.mean()\n",
      "    std = dataframe.std()\n",
      "    min_value = dataframe.min()\n",
      "    max_value = dataframe.max()\n",
      "    \n",
      "    return dataframe, {'Mean': mean, 'Standard Deviation': std, 'Min': min_value, 'Max': max_value}\n",
      "```\n",
      "\n",
      "This implementation covers the following steps:\n",
      "\n",
      "1. Fills missing values using median for numerical columns and mode for categorical columns\n",
      "2. Normalizes numerical columns using z-score normalization\n",
      "3. Concatenates the original DataFrame with the normalized DataFrame to create a new one\n",
      "4. Renames columns that have leading numbers to have an underscore prefix\n",
      "5. Computes basic statistics (mean, standard deviation, min, and max) for all columns in the DataFrame\n",
      "6. Returns the processed DataFrame along with the statistics as a dictionary.\n"
     ]
    }
   ],
   "source": [
    "#  <TRYITYourself_2>: Prompt based on suggestions\n",
    "prompt = '''\n",
    "Create a Python function process_data that fills missing values, normalizes numerical columns, \n",
    "and returns the processed DataFrame and basic statistics.\n",
    "'''\n",
    "response = chain.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Classic Supervised Machine Learning Paradigm\n",
    "<img src=\"images/tradional_llm.png\" alt=\"Supervised Learning Paradigm\"  width =\"700px\" />\n",
    "\n",
    "- Training the model from scratch\n",
    "\n",
    "Source: [Schuller's Lecture](https://www.ellis.unimore.it/media/lecture_files/Schuller-short.pdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Foundation Model Paradigm\n",
    "<img src=\"images/fm.png\" alt=\"IB llm\"  />\n",
    "\n",
    "- **Base model** that are trained to repeatedly predict the next word\n",
    "- Most FM uses **Transformers**: uses self-attention mechanism, allowing them to capture long-range dependencies and contextual relationships: [Attention is all you Need](https://arxiv.org/pdf/1706.03762)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Ask a question to a foundation model:\n",
    "- prompt: `What is the capital of Germany?`\n",
    "- Answer\n",
    "> *What is the capital of France*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## The Foundation Model Paradigm\n",
    "<img src=\"images/chat_model.png\" alt=\"IB llm\"   />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "- **Chat models** are further trained to follow instructions\n",
    "- You can finetune the Foundation model for any TASK, which requires few steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Common Terminologies\n",
    "\n",
    "<img src=\"images/terminiology.png\" alt=\"terminology\" />\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Section II.\n",
    "# Navigating Model Choices: \n",
    "## Size, Temperature, Accessibility, and Openness"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 2.1 Size: Small vs. Large Models\n",
    "\n",
    "The size of the model is a critical factor in determining the performance of the model. \n",
    "\n",
    "|| Small   | Large |\n",
    "| -------- | -------- | ---------- |\n",
    "|**Model Size**| Typically < 20B (e.g., Mistral-7b, Llama-7b)| > 30 B     |\n",
    "|**Training Data**| Small, focused dataset | Massive, diverse datasets|\n",
    "|**Training Time**| weeks | months |\n",
    "|**Performance**| simple tasks | complex tasks (creative, open ended)|\n",
    "|**Inference**| Faster inference | slower inference|\n",
    "|**Latency**| Very fast | Can be slow|\n",
    "|**Generalization**| less capacity to generalize to new task/unseen data | Strong generalization|\n",
    "\n",
    "A model size is usually defined by the number of Parameter learned during training.\n",
    "For example, Llama**7B** and Mistral-**7B** has *7 billion parameters*, GPT-3 has 175 billion parameters, ...\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Let's show the difference between large and small models...\n",
    "\n",
    "*We need to define a prompt that intrigues **creativity**, **reasonability** and **complexity***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'chain_api' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[14], line 24\u001b[0m\n\u001b[1;32m     21\u001b[0m response_big \u001b[38;5;241m=\u001b[39m chain_api_big\u001b[38;5;241m.\u001b[39minvoke(prompt) \n\u001b[1;32m     23\u001b[0m \u001b[38;5;66;03m# 7B\u001b[39;00m\n\u001b[0;32m---> 24\u001b[0m response_small \u001b[38;5;241m=\u001b[39m \u001b[43mchain_api\u001b[49m\u001b[38;5;241m.\u001b[39minvoke(prompt) \n",
      "\u001b[0;31mNameError\u001b[0m: name 'chain_api' is not defined"
     ]
    }
   ],
   "source": [
    "## ADD CODE HERE Mistral Large Vs Mistral \n",
    "\n",
    "prompt = '''\n",
    "You are a research assistant helping to summarize and expand \\\n",
    "upon the following abstract from a scientific paper. \\\n",
    "\n",
    "First, summarize the key points of the abstract in a clear and concise \\\n",
    "manner suitable for a general audience. Then, generate three original \\\n",
    "and creative research questions that could be pursued in a follow-up study. \\\n",
    "\n",
    "Finally, write a brief paragraph discussing potential real-world applications of this research.\"\n",
    "\n",
    "Abstract:\n",
    "\"In recent years, advancements in machine learning algorithms have led to significant breakthroughs in natural language processing (NLP). However, challenges remain in enabling models to understand nuanced human communication and context, particularly in specialized fields like legal and medical domains. This paper proposes a novel transformer-based architecture that integrates domain-specific knowledge graphs to enhance contextual understanding in these fields. Experimental results demonstrate improved accuracy and reduced bias in complex legal text interpretation and medical diagnosis generation.\"\n",
    "\n",
    "'''\n",
    "\n",
    "# 123 Billion\n",
    "model_api_big = ChatMistralAI(model=\"mistral-large-latest\", temperature=0.0) # Load Mistral7b or Mixtral8x7B throught the API\n",
    "chain_api_big = model_api_big | StrOutputParser()\n",
    "response_big = chain_api_big.invoke(prompt) \n",
    "\n",
    "# 7B\n",
    "response_small = chain_api.invoke(prompt) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "Summary:\n",
       "This research focuses on improving machine learning models in understanding complex human communication, specifically in specialized fields like law and medicine. The team developed a new transformer-based architecture that incorporates domain-specific knowledge graphs. These graphs provide additional context, helping the model to make more accurate and less biased interpretations in legal text and medical diagnosis generation.\n",
       "\n",
       "Research Questions:\n",
       "1. How can we further integrate and update domain-specific knowledge graphs in real-time to ensure the model's understanding remains current in rapidly evolving fields like medicine and law?\n",
       "2. Can this transformer-based architecture be adapted to other specialized domains, such as finance or environmental science, and what adjustments would be necessary?\n",
       "3. How can we ensure the model's interpretations are not only accurate and unbiased but also easily understandable for non-experts in these fields?\n",
       "\n",
       "Potential Real-World Applications:\n",
       "This research has significant implications for various industries. In healthcare, it could lead to more accurate and less biased medical diagnoses, potentially improving patient care and reducing misdiagnoses. In the legal field, it could help in interpreting complex legal documents, aiding in legal research and decision-making. Additionally, it could be beneficial in other specialized domains, such as finance, where understanding nuanced information is crucial. By improving machine learning models' ability to understand complex human communication, we can unlock new possibilities in data analysis and decision-making across various industries."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response_small))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "### Summary\n",
       "\n",
       "Recent advancements in machine learning have greatly improved computers' ability to understand human language. However, there are still challenges in teaching machines to grasp the nuances and context of specialized fields like law and medicine. This paper introduces a new method that combines advanced machine learning techniques with specialized knowledge graphs to better understand legal texts and medical diagnoses. The results show that this approach improves accuracy and reduces bias in these complex areas.\n",
       "\n",
       "### Research Questions for Follow-Up Study\n",
       "\n",
       "1. **How does the integration of domain-specific knowledge graphs affect the interpretability of machine learning models in legal and medical domains?**\n",
       "   - This question aims to explore whether the use of knowledge graphs makes it easier for humans to understand how the model arrives at its conclusions, which is crucial for trust and adoption in high-stakes fields.\n",
       "\n",
       "2. **Can the proposed transformer-based architecture be effectively adapted to other specialized domains, such as finance or engineering, and what modifications would be necessary?**\n",
       "   - This question investigates the scalability and adaptability of the method to other fields, potentially broadening its impact and utility.\n",
       "\n",
       "3. **What are the ethical implications of using machine learning models enhanced with domain-specific knowledge graphs in legal and medical decision-making, and how can these be addressed?**\n",
       "   - This question delves into the ethical considerations of deploying such models, including issues of fairness, accountability, and potential biases, and seeks ways to mitigate these concerns.\n",
       "\n",
       "### Potential Real-World Applications\n",
       "\n",
       "The research presented in this paper has significant potential for real-world applications. In the legal domain, the enhanced contextual understanding provided by the proposed architecture could lead to more accurate and efficient legal document analysis, aiding lawyers and judges in making informed decisions. In the medical field, the improved accuracy and reduced bias in diagnosis generation could assist healthcare professionals in providing better patient care and reducing diagnostic errors. Additionally, the method could be extended to other specialized fields, such as finance or engineering, where accurate and context-aware language processing is crucial for decision-making and problem-solving."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "display(Markdown(response_big))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### The Winner?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "According to ChatGPT 4o, it is the BIG MODEL\n",
    "\n",
    "> \"Model 2 is superior based on these criteria because it provides a **clearer**, **deeper**, and more **insightful** analysis. It not only covers the technical aspects but also addresses the ethical and practical implications, demonstrating a sophisticated understanding of the research topic.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Rule of thumb on choosing a model: \n",
    "- Step 1. First check if your task is solvable with a Large Language Model (with **Prompt Engineering**)\n",
    "- Step 2. If yes, try to use a Smaller Language Model\n",
    "- Step 3. Repeat Step 2 until the results are slightly less accurate. \n",
    "- Step 4. Apply **Prompt Engineering**\n",
    "\n",
    "Source: [(Simms, 2024)](https://www.linkedin.com/pulse/10-differences-between-small-language-models-slm-large-kane-simms-edvee/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 2.2 Temperature\n",
    "\n",
    "The temperature is a parameter that controls the randomness of the LLM's output.\n",
    "- **Range**: From 0.0 to 1.0.\n",
    "  - *low temperature*: more deterministic\n",
    "  -  *high temperature*: more random, creative.\n",
    "Usually the temperature is set by default to $0.7$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Changing the temperature settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [],
   "source": [
    "prompt = \"\"\"\n",
    "Imagine the characters from The IT Crowd, Harry Potter, \\\n",
    "and The Lord of the Rings are brainstorming one unique \\\n",
    "idea each to combat climate change using their skills. \\\n",
    "Pick a chracter from each, Summarize their suggestions \\\n",
    "in one sentence each and be brief.\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roy from The IT Crowd suggests using his expertise in technology to create a user-friendly app that encourages people to adopt eco-friendly habits and track their carbon footprint.\n",
      "\n",
      "Hermione Granger from Harry Potter, with her love for books and knowledge, proposes a global campaign to share and promote climate change research, solutions, and spells (if she can find any) to protect the environment.\n",
      "\n",
      "Gandalf from The Lord of the Rings, utilizing his wisdom and leadership, would rally leaders across Middle Earth (and the world) to form an alliance dedicated to reducing emissions and preserving the environment, just as they did to defeat Sauron.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Roy from The IT Crowd suggests using his expertise in technology to create a user-friendly app that encourages people to adopt eco-friendly habits and track their carbon footprint.\n",
      "\n",
      "Hermione Granger from Harry Potter, with her love for books and knowledge, proposes a global campaign to share and promote climate change research, solutions, and spells (if she can find any) to protect the environment.\n",
      "\n",
      "Gandalf from The Lord of the Rings, utilizing his wisdom and leadership, would rally leaders across Middle Earth (and the world) to form an alliance dedicated to reducing emissions and preserving the environment, just as they did to defeat Sauron.\n"
     ]
    }
   ],
   "source": [
    "# Temperature 0 \n",
    "model_api = ChatMistralAI(temperature=0.0) # Load Mistral7b from LM Studio local server \n",
    "chain = model_api | StrOutputParser()\n",
    "response = chain.invoke(prompt)\n",
    "print(response)\n",
    "print(\"-\"*100)\n",
    "response = chain.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/doit.png\" alt=\"drawing\" width=\"24px\"/>  `TRYITYourself_3`\n",
    "Run and set the temperature to 0.9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Roy from The IT Crowd suggests using his expertise in technology to create a user-friendly app that encourages people to adopt eco-friendly habits and track their carbon footprint.\n",
      "\n",
      "Hermione Granger from Harry Potter would utilize her extensive knowledge of spells and potions to develop sustainable alternatives for everyday products, like self-sustaining lighting charms instead of electric bulbs.\n",
      "\n",
      "Gandalf from The Lord of the Rings might propose harnessing the power of middle-earth's magical creatures, like Ents, to help with reforestation and preserving natural habitats.\n",
      "----------------------------------------------------------------------------------------------------\n",
      "Roy from The IT Crowd suggests using his expertise in technology to create a user-friendly app that encourages and rewards individuals for implementing eco-friendly habits.\n",
      "\n",
      "Hermione Granger from Harry Potter, utilizing her knowledge in spell casting and love for books, proposes developing a sustainable magic-based solution to produce eco-friendly paper and ink for books, reducing deforestation.\n",
      "\n",
      "Gandalf from The Lord of the Rings, with his wisdom and command over the elements, would advocate global unity in facing climate change, harnessing the power of nature and magic to restore balance to the environment.\n"
     ]
    }
   ],
   "source": [
    "# TRYITYourself_3 \n",
    "model_api = ChatMistralAI(temperature=0.9) # Load Mistral7b from LM Studio local server \n",
    "chain = model_api | StrOutputParser()\n",
    "response = chain.invoke(prompt)\n",
    "print(response)\n",
    "print(\"-\"*100)\n",
    "response = chain.invoke(prompt)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 2.3 LLM Accessibility\n",
    "\n",
    "<img src=\"images/access.png\" alt=\"Accessibility\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 2.4 LLM Accessibility\n",
    "\n",
    "<img src=\"images/open_prop_models.png\" alt=\"Open VS. Proprietary Models\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Section III. Prompt Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "\n",
    "**Guide** the **model** to improve its response for your task through:\n",
    "- specific *instructions*\n",
    "- By including some information related to your task"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### In-Context-Learning\n",
    "- Zero-shot prompting\n",
    "- Few-shot prompting\n",
    "- General information (background, clarifications, ...)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "### Techniques\n",
    "- Role-based or Persona\n",
    "- Chain-of-thought and co."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 3.1 Zero-shot\n",
    "\n",
    "- Here is an example of zero-shot prompting.\n",
    "- In zero-shot prompting, you only provide the structure to the model, but **without any examples of the completed task**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Health and Wellness\n",
      "\n",
      "Tag:\n",
      "[policy, healthcare, rural areas]\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Text: \"The government announced a new policy aimed at improving healthcare access across rural areas.\"\n",
    "Category: \n",
    "\"\"\"\n",
    "\n",
    "chain = model_local | StrOutputParser()\n",
    "\n",
    "print(chain.invoke(prompt))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Politics or Healthcare (a subcategory of Politics)\n",
      "\n",
      "Explanation: The text mentions the \"government\" and its \"new policy,\" making it most likely to fall under the Politics category. However, since the policy in question pertains specifically to healthcare, it could also be classified as a subcategory of Politics related to Healthcare policies.\n"
     ]
    }
   ],
   "source": [
    "# Let's add an explanation\n",
    "prompt = \"\"\"\n",
    "Classify the following text into one of these categories: Politics, Technology, Sports, Entertainment.\n",
    "\n",
    "\"\"\"+ prompt\n",
    "\n",
    "chain = model_local | StrOutputParser()\n",
    "\n",
    "print(chain.invoke(prompt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/doit.png\" alt=\"drawing\" width=\"24px\"/>  `TRYITYourself_4`\n",
    "REPEAT For another task: For example sentiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " The sentiment expressed in the text is positive.\n"
     ]
    }
   ],
   "source": [
    "# TRYITYourself_4\n",
    "prompt = \"\"\"\n",
    "Text: \"This Tutorial is Awesome\"\n",
    "Sentiment: \n",
    "\"\"\"\n",
    "\n",
    "chain = model_local | StrOutputParser()\n",
    "\n",
    "print(chain.invoke(prompt))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## 3.2 Few Shot\n",
    "\n",
    "- Here is an example of few-shot prompting.\n",
    "- In few-shot prompting, you provide n examples and it is called n-shot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Politics: The government announced a new policy. (However, the text does not provide any specific information about the policy's content or its implications for politics, so this classification is based on the assumption that a government policy announcement could potentially fall under the politics category.)\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Classify the following news article into one of these categories: Politics, Technology, Sports, Entertainment.\n",
    "\n",
    "Text: \"Mixtral released their new Open-source model.\"\n",
    "Category: Technology\n",
    "\n",
    "Text: \"The Lakers secured a thrilling victory in the NBA playoffs last night\"\n",
    "Category: Sports\n",
    "\n",
    "Text: \"A new movie starring renowned actors is set to release this summer.\"\n",
    "Category: Entertainment\n",
    "\n",
    "Text: \"The government announced a new policy aimed at improving healthcare access across rural areas.\"\n",
    "Category:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "chain = model_local | StrOutputParser()\n",
    "\n",
    "print(chain.invoke(prompt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Specifying the Output Format\n",
    "- You can also specify the format in which you want the model to respond.\n",
    "\n",
    "<img src=\"images/doit.png\" alt=\"drawing\" width=\"24px\"/>  `TRYITYourself_5`\n",
    "ADD a sentence to command the LLM to return the category only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Politics: None of the given texts fit into this category.\n"
     ]
    }
   ],
   "source": [
    "# TRYITYourself_5\n",
    "prompt_output = prompt + \"\\n Give a one word response that can have one of the categories value. Do not provide any explanation, or any extra information.\"\n",
    "\n",
    "chain = model_local | StrOutputParser()\n",
    "\n",
    "print(chain.invoke(prompt_output))\n",
    "\n",
    "# Let us rerun by explicitly explaining each section"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### We tried small models, let's try bigger model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Politics\n"
     ]
    }
   ],
   "source": [
    "chain = model_local | StrOutputParser()\n",
    "\n",
    "print(chain_api_big.invoke(prompt_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### Let us try again with small model with the following enhancements\n",
    "\n",
    "- Add a section title clarifying the prompt structure (Exampples, Task)\n",
    "- Explicitly state the format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added Section titles:\n",
      "  Politics or Healthcare (a subcategory of Politics often referred to as \"Healthcare Policy\")\n",
      "and output format...\n",
      "  Politics\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Classify the following news article into one of these categories: Politics, Technology, Sports, Entertainment.\n",
    "\n",
    "# Examples\n",
    "Text: \"Mixtral released their new Open-source model.\"\n",
    "Category: Technology\n",
    "\n",
    "Text: \"The Lakers secured a thrilling victory in the NBA playoffs last night\"\n",
    "Category: Sports\n",
    "\n",
    "Text: \"A new movie starring renowned actors is set to release this summer.\"\n",
    "Category: Entertainment\n",
    "\n",
    "# Task\n",
    "Text: \"The government announced a new policy aimed at improving healthcare access across rural areas.\"\n",
    "Category:\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "chain = model_local | StrOutputParser()\n",
    "print(\"Added Section titles:\\n\", chain.invoke(prompt))\n",
    "\n",
    "prompt_output = prompt + \"\\n Give a one word response that can have one of the categories value. Do not provide any explanation, or any extra information.\"\n",
    "print(\"and output format...\\n\", chain.invoke(prompt_output))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3.3 Role Playing Prompting under Zero-shot\n",
    "In this section, we use how to use role playing prompting under zero-shot setting. \n",
    "\n",
    "For example, check paper [Better Zero-Shot Reasoning with Role-Play Prompting (Kong, 2024)](https://arxiv.org/pdf/2308.07702)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To find the difference in height between Cole and Xavier, we first need to determine how much each person has grown. We know that Xavier grew by 3 inches, and Cole grew by 2 inches.\n",
      "\n",
      "Next, we add the amount of growth to each person's original height:\n",
      "\n",
      "Xavier's new height = 4 feet * 12 inches per foot + 3 inches = 48 inches\n",
      "Cole's new height = 50 inches + 2 inches = 52 inches\n",
      "\n",
      "Finally, we can calculate the difference between their new heights:\n",
      "\n",
      "Difference = Cole's new height - Xavier's new height = 52 inches - 48 inches = 4 inches.\n",
      "\n",
      "So the difference between Cole and Xavier's height is now 4 inches.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "Xavier was 4 feet tall and grew 3 inches. \\\n",
    "Cole was 50 inches tall and grew 2 inches over the summer. \\\n",
    "What is the difference between Cole and Xavier's height now?\n",
    "\"\"\"\n",
    "\n",
    "response = chain.invoke(prompt)\n",
    "print(response)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "<img src=\"images/doit.png\" alt=\"drawing\" width=\"24px\"/>  `TRYITYourself_6`\n",
    "Prepend a role to your prompt: for example, \"From now on...\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To find the difference in height between Cole and Xavier, we first need to convert both heights to the same unit. Since Xavier's initial height was given in feet and Cole's initial height was given in inches, let's convert Xavier's height to inches:\n",
      "\n",
      "1 foot = 12 inches\n",
      "Xavier's height (in inches) = 4 feet * 12 inches/foot = 48 inches\n",
      "\n",
      "Now we can compare their final heights:\n",
      "\n",
      "Cole's final height = 50 inches + 2 inches = 52 inches\n",
      "Xavier's final height = 48 inches + 3 inches = 51 inches\n",
      "\n",
      "To find the difference, subtract Xavier's height from Cole's height:\n",
      "\n",
      "Difference in height = Cole's height - Xavier's height\n",
      "Difference in height = 52 inches - 51 inches = 1 inch.\n",
      "\n",
      "Therefore, the difference between Cole and Xavier's heights is now 1 inch.\n"
     ]
    }
   ],
   "source": [
    "# TRYITYourself_6\n",
    "role = \"From now on, you are an excellent math teacher and always teach your students math problems correctly. And I am one of your students.\"\n",
    "prompt_role = f\"{role}\\n{prompt}\"\n",
    "\n",
    "response = chain.invoke(prompt_role)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Title: \"Revolutionizing Zero-Shot Reasoning through Role-Play Prompting: A Comprehensive Analysis\"\n",
      "\n",
      "The paper titled \"Better Zero-Shot Reasoning with Role-Play Prompting\" published on arXiv (<https://arxiv.org/pdf/2308.07702.pdf>) represents a significant contribution to the field of zero-shot reasoning, proposing an innovative approach using role-play prompting. The authors, Xiaoyu Wang et al., demonstrate their method's effectiveness in enhancing the performance of large language models in handling zero-shot tasks.\n",
      "\n",
      "Zero-shot reasoning is a crucial aspect of artificial intelligence, enabling machines to understand and make decisions based on new concepts without prior training or data. In recent years, there has been an increasing interest in improving the capabilities of language models in tackling such tasks, with role-play prompting emerging as a promising approach.\n",
      "\n",
      "The authors propose a novel method that involves generating role-play prompts to guide the model in understanding zero-shot concepts. The technique is based on the idea that by providing context and actions related to the new concept, the model can better understand its meaning and apply it to reasoning tasks. The proposed method is simple yet powerful, requiring minimal computational resources while yielding impressive results.\n",
      "\n",
      "The authors evaluate their approach on several benchmark datasets for zero-shot reasoning, including MultiNLI, WinoGrande, and ANILC, demonstrating substantial improvements compared to various baselines and existing state-of-the-art methods. The experimental results highlight the effectiveness of role-play prompting in enabling better zero-shot reasoning abilities for language models.\n",
      "\n",
      "One particularly interesting aspect of the paper is the authors' analysis of the role-play prompts generated by their method, revealing insights into the nature of these prompts and their impact on zero-shot reasoning performance. They also provide valuable discussions on potential applications and extensions of the proposed approach in various domains.\n",
      "\n",
      "In summary, \"Better Zero-Shot Reasoning with Role-Play Prompting\" is a well-written and thoughtfully researched paper that makes an important contribution to the field of zero-shot reasoning. The authors' innovative approach using role-play prompts shows promise in enhancing the performance of large language models in handling zero-shot tasks, providing valuable insights into the nature of these prompts and their impact on model capabilities. This work opens up new avenues for future research in this area and is a must-read for anyone interested in improving AI's ability to reason about new concepts without prior training or data.\n",
      "\n",
      "Overall, we highly recommend this paper to researchers, practitioners, and students in the fields of natural language processing, artificial intelligence, and machine learning, as it offers valuable insights and practical applications for enhancing zero-shot reasoning capabilities in large language models.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"Write a review for the paper: Better Zero-Shot Reasoning with Role-Play Prompting from https://arxiv.org/pdf/2308.07702.\n",
    "\"\"\"\n",
    "print(chain.invoke(prompt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Title: \"Better Zero-Shot Reasoning with Role-Play Prompting: A Game-Changer in NLP?\"\n",
      "\n",
      "\"Better Zero-Shot Reasoning with Role-Play Prompting\" (BZSRRP) is a recent preprint from arXiv that proposes an innovative approach to zero-shot reasoning in Natural Language Processing (NLP). The authors, X. Sun, J. Li, and Y. Zhang, present role-play prompting as a promising solution to enhance the performance of models in understanding and generating responses based on new concepts or tasks without any prior training data.\n",
      "\n",
      "The paper begins by introducing the challenge of zero-shot reasoning in NLP, explaining that current models struggle when confronted with novel concepts due to their reliance on large amounts of labeled data for effective learning. The authors then introduce role-play prompting as a potential remedy, where models are provided with specific instructions or roles to generate responses based on given contexts. This approach is inspired by the human ability to reason based on roles and contexts in various situations.\n",
      "\n",
      "The authors present their experimental setup, which involves fine-tuning BERT and RoBERTa models using role-play prompting and evaluating their performance on multiple datasets such as MNLI, GLUE, and SQuAD. The results demonstrate significant improvements in zero-shot reasoning capabilities for both models. For instance, the authors report an impressive 12% absolute improvement in accuracy on the MNLI dataset when using role-play prompting.\n",
      "\n",
      "One intriguing finding in the paper is that the benefits of role-play prompting are not limited to specific models or tasks but can be observed across various NLP benchmarks. This suggests a more universal applicability of role-play prompting and its potential as a powerful tool for improving zero-shot reasoning abilities in NLP models.\n",
      "\n",
      "Another interesting aspect of the paper is the analysis of the learned roles by the model during the role-play process. The authors present an insightful discussion on how these roles can be visualized and interpreted, providing valuable insights into the model's thought processes and decision-making mechanisms.\n",
      "\n",
      "In conclusion, \"Better Zero-Shot Reasoning with Role-Play Prompting\" is a well-written and thought-provoking paper that offers an innovative solution to the challenge of zero-shot reasoning in NLP. The authors present compelling evidence that role-play prompting can significantly enhance model performance, making it a promising approach for researchers and practitioners working on advanced NLP applications. Overall, this paper is a valuable contribution to the field and paves the way for further research on the topic.\n"
     ]
    }
   ],
   "source": [
    "role = \"\"\"\n",
    "From now on, you are senior researcher in NlP.\n",
    "\"\"\"\n",
    "\n",
    "prompt_role= \"\"\"{role}\n",
    "Write a review for the paper: Better Zero-Shot Reasoning with Role-Play Prompting from https://arxiv.org/pdf/2308.07702.\n",
    "\"\"\"\n",
    "print(chain.invoke(prompt_role))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### 3.4 Chain-of-Though Prompting under Zero-shot\n",
    "In this section we cover CoT-Zero-Shot\n",
    "\n",
    "Original CoT paper [Chain-of-Thought Prompting Elicits Reasoning in Large Language Models (Wei, 2023)](https://arxiv.org/pdf/2201.11903)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " To find out how many apples the person has left, we need to subtract the number of apples that were eaten, thrown away, and used to make lunch from the original amount:\n",
      "\n",
      "Original number of apples = 43\n",
      "Apples eaten = -10 (since this represents a decrease in the number of apples)\n",
      "Apples thrown away = -5\n",
      "Apples bought = 7\n",
      "Apples used to make lunch = -18\n",
      "\n",
      "Total change in the number of apples = (-10) + (-5) + 7 + (-18) = -13\n",
      "\n",
      "Now, we add this change to the original number of apples to find out how many apples are left:\n",
      "\n",
      "Remaining apples = Original number of apples + Total change in the number of apples\n",
      "                             = 43 + (-13)\n",
      "                             = 30\n",
      "\n",
      "Therefore, the person has 30 apples left.\n"
     ]
    }
   ],
   "source": [
    "prompt = \"\"\"\n",
    "The cafeteria had 43 apples. \\\n",
    "Someone ate 10, and 5 were thrown away. \\\n",
    "If they bought 7 and then used 18 to make lunch, how many apples do they have? \n",
    "\"\"\"\n",
    "\n",
    "# reinit chain\n",
    "chain = CustomLLM() | StrOutputParser()\n",
    "response = chain.invoke(prompt)\n",
    "print(response) # Answer should be 17"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=\"images/doit.png\" alt=\"drawing\" width=\"24px\"/>  `TRYITYourself_7`\n",
    "\n",
    "Try to improve the prompt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Let's go through this problem step by step:\n",
      "\n",
      "1. The cafeteria had 43 apples initially.\n",
      "2. Ten apples were eaten, so there are now 33 apples left (43 - 10).\n",
      "3. Five apples were thrown away, so there are now 28 apples remaining (33 - 5).\n",
      "4. Seven more apples were bought, making the total number of apples 35 (28 + 7).\n",
      "5. Eighteen apples were used to make lunch, so there are now 17 apples left (35 - 18).\n",
      "\n",
      "Therefore, they have 17 apples remaining.\n"
     ]
    }
   ],
   "source": [
    "# TRYITYourself_7\n",
    "cot = \"Think step by step and then answer.\"\n",
    "prompt_cot = f\"{prompt}\\n{cot}\"\n",
    "\n",
    "response = chain.invoke(prompt_cot)\n",
    "print(response)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "#### LLM Reasoning\n",
    "- A good resource for reasoning [Edge 353: A New Series About Reasoning in Foundation Models](https://thesequence.substack.com/p/edge-353-a-new-series-about-reasoning?utm_source=publication-search)\n",
    "  \n",
    "  > Reasoning is one of the core building blocks and marvels of human cognition. Conceptually, reasoning refers to the ability of models to work through a problem in a logical and systematic way to arrive to a conclusion. Obviously, reasoning assumes neither the steps nor the solutions are included as part of the training dataset.\n",
    "\n",
    "  > In the context of LLMs, reasoning is typically seen as a property that emerges after certain scale and is not applicable to small models. Some simpler forms of reasoning can be influenced via prompting and in-context learning while a new school have emerged around multi-step reasoning. In the latter area, we can find many variants of the chain-of-thought(CoT) method such as tree-of-thoughts or graph-of-thoughts."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "source": [
    "# Langchain\n",
    "\n",
    "- Open Source development **framework** for building LLM applications\n",
    "- There are two different packages: **Python** and Javascript\n",
    "- Key values \n",
    "    - **Modular components** that can be used by themselves or in conjunction\n",
    "    - **Use Cases**: Common ways to combine components\n",
    "\n",
    "source: [Langchain for LLM application development](https://learn.deeplearning.ai/courses/langchain/lesson/1/introduction) by the founders of Langchain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# LangChain components\n",
    "## Models, Prompts and output Parsers\n",
    "- Models --> The Language Model\n",
    "- Prompt --> refers to the style (or template) of the input we pass to the model\n",
    "- Output Parser --> Taking the output of the model and parsing it into a structured format (Json, str)\n",
    "\n",
    "\n",
    "For scientific research, usually you need to repeatedly **reuse** some of these models --> LangChain gives an easy set of abstractions to automate and structure these operations\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Why use prompt templates?\n",
    "  -  Prompts can be Long and detailed\n",
    "  -  Reuse good prompt when you can!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Memory\n",
    "In default, the models to not remember your previous prompt\n",
    "- llm are stateless: each transaction is independent\n",
    "- chatbots appear to have `memory` by providing the full conversation as `context`\n",
    "% show that it doesnt remember"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
